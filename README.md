# **📖 Aura Reads 🎶**

## Project Overview

I was talking few years back about this idea with a close friend of mine who is an avid reader. He always listens to some music while reading a book (I, for one, prefer reading in silence) and was frustrated because each time he started a new book, a new playlist was necessary because the vibe of the book changed 🙄.
How about an ebook reader that chose and played music for you while you are reading, adapting to the content of the page? At that time (a few years ago) it seemed like a very complex idea, but now, with the development and mass availability of LLMs, this idea starts to be very approachable.

I called this app **Aura Reads** (the name can change down the road, of course). It will be a modern ebook reader web application designed to create a more immersive and personalized reading experience for those that prefer listening to music while reading.
The core idea is to dynamically generate a musical soundtrack that adapts in real-time to the emotional tone of the narrative.

## Use Case & Workflow

The primary user is any reader who enjoys music and wants to deepen their connection to the books they love. The workflow is designed to be seamless, with the AI components working in the background to enhance the experience.

**Workflow:**

1.  **👤 Onboarding & Profile Creation:** A user signs up and can optionally connect their Spotify account. To build their literary profile, they can upload ebooks (e.g., ePub files) and connect to external libraries like Goodreads or Google Books to import their reading history.

2.  **🤖 AI-Powered Analysis (Backend):**

    -   When a book is added, the backend parses it into raw text and then into logical chunks (paragraphs or groups of paragraphs so that a chunck can be around 1 minute of reading time for the average reader, or, to create an even more personalized approach, around 1 minute for the user based on their age and their previous reading speed recorded by the application).

    -   These chunks are then processed by a **LLM** running on a dedicated server. Using carefully crafted prompts, the LLM analyzes each chunk and returns a structured JSON object that contains some metadata ('mood', 'sentiment', 'type' etc.) This metadata is stored in a PostgreSQL database.

    -   This process is the core of the AI automation, as it systematically deconstructs an entire novel into an emotional and structural map without manual intervention.

3.  **🎧 The Immersive Reading Experience:**

    -   The user opens a book in the reader interface. As they scroll, the application tracks their position.

    -   The app identifies the pre-analyzed mood and sentiment of the current text chunk.

    -   This textual analysis is then combined with the user's RAG-generated profile (their literary and musical tastes).

    -   2 modes will be available:

        -   A 'Music library' mode. A highly specific request is sent to the **Spotify API** to find a song that matches both the text's mood and the user's personal preferences.

        -   An 'Ambiental' mode that will not be connected to any music service, instead, a library of ambiental sounds will be used.
            For this library, songs will be generated by AI for every specific mood, sentiment, type of a chunk, for a more personalized experience, these songs can be generated for each user and for each book that the user owns.
            Based on the mood and the user profile then a specific sound will be selected

        The resulting track plays in the background. To ensure a smooth experience, the music only changes when the mood of the narrative shifts significantly, at which point it will seamlessly crossfade to the new track.

## AI Features to Be Implemented

-   **📝 Prompt Engineering & Structured Outputs:**
    This is fundamental to the application's logic. I will use the LLM's structured output capability (specifically `"application/json"`) to ensure the text analysis returns clean, predictable JSON. A list of possible values for each field will be specified as context for each prompt (ex: for `mood` some possible values will be: `happy`,`sad`,`angry`, etc.).

    This is critical because this structured data is the direct input for the music recommendation logic. Its relevance is in providing reliability and eliminating the need for fragile string parsing.

-   **📊 Retrieval-Augmented Generation (RAG):**

    The user will have a profile, stored in the database and it will have 2 components

    -   their literary tastes (from their library, from a form of previous books read or from an integration with a third party books app like Google Books or GoodReads)
    -   musical preferences (from Spotify, if the user chooses to sync their Spotify account with the Aura Reads app)
    -   **Retrieved Context** will look something like this:

        ```json
        {
        	"user_id": 12,
        	"literary_tastes": {
        		"genres": ["historical fiction", "fantasy"],
        		"authors": ["George R.R. Martin"]
        	},
        	"musical_preferences": {
        		"genres": ["indie rock", "orchestral"],
        		"artists": ["Arctic Monkeys", "Radiohead", "Hans Zimmer"]
        	}
        }
        ```

    This profile will be used as context to "augment" the music selection process. Instead of just asking for a "sad song," the system asks for a "sad song that a fan of indie rock and historical fiction would enjoy." This feature transforms generic mood-matching into a more personalized experience.

## Technical Approach

I will build this application using a modern, type-safe technology stack that I am proficient with.

-   **Frontend:** A responsive single-page application built with **React** and **TypeScript**. I will use the **Mantine UI** component library for a clean and accessible design and **Tanstack React Query** for efficiently managing server state.

-   **Backend:** A **.NET** minimal web API will serve as the application's core, handling user authentication, business logic, and communication with the database and **Python** scripts for LLM API calls and prompt processing.

-   **Database:** I will use a **PostgreSQL** database to store user data, ebook content, and all AI-generated metadata (moods, sentiments, user profiles).

-   **AI & Scripting:**

    -   A **local LLM** will be hosted on a separate server to ensure user data privacy and copyright compliance. Considering the complexity of the task, a local LLM should be appropriate for the task (general models such as `Llama 3 8B`, `Mistral 7B`)

    -   **Python scripts** will be used to interface with the LLM, leveraging its powerful libraries for generative AI tasks.

-   **Development**

    -   I will use **Cursor** as my primary IDE to leverage its AI-powered features and boost development speed.

    -   For code quality and automated reviews, I plan to integrate **CodeRabbit** into the development workflow.

## Example Prompts & Expected Outputs

### 1. Text Chunk Analysis

**Example Prompt (to the local LLM):**

```
Analyze the mood, sentiment, type, and type_details of the following paragraphs:

The output should be in the following format as a JSON array:
    paragraph_id: [id of the paragraph]
    mood: [mood of the paragraph; possible values: happy, sad, angry, excited, anxious, confused, surprised, disappointed, grateful, lonely, hopeful, content, frustrated]
    sentiment: [sentiment of the paragraph; possible values: positive, negative, neutral]
    type: [type of the paragraph; possible values: dialogue, description, action, reflection]
    type_details: [details of the type; ex: if the paragraph is a description of nature, the type_details will be "nature"]

    Paragraphs:
    [Paragraph with id=1]: "The sun was shining brightly, and the birds were singing."
    [Paragraph with id=2]: "The trees burned down in the fire. Animals were running around in fear."
    [Paragraph with id=3]: ""I can't believe you did that," she whispered, her voice trembling. "After everything we've been through.""
```

-   **Expected Structured Output (JSON):**

    ```json
    [
    	{
    		"paragraph_id": 1,
    		"mood": "happy",
    		"sentiment": "positive",
    		"type": "description",
    		"type_details": "nature"
    	},
    	{
    		"paragraph_id": 2,
    		"mood": "sad",
    		"sentiment": "negative",
    		"type": "description",
    		"type_details": "disaster"
    	},
    	{
    		"paragraph_id": 3,
    		"mood": "disappointed",
    		"sentiment": "negative",
    		"type": "dialogue",
    		"type_details": "conflict"
    	}
    ]
    ```

### 2. RAG-Based Recommendation Mapping

This isn't a single prompt but a logic flow.

1.  **Input from Text Analysis:** `{ "mood": "sad", "sentiment": "negative", "type": "contemplation" }`

2.  **Input from RAG (User Profile):** "User reads a lot of `fantasy` books and enjoys `indie rock` and `orchestral` music.
    Top artists include `Arctic Monkeys`, `Radiohead` and `Hans Zimmer`."

3.  **Logic:**

    -   For the 'Music library' mode, map these inputs to Spotify API parameters.

        -   `mood: "sad"` -> `target_valence: 0.2`

        -   `type: "contemplation"` -> `target_energy: 0.3`, `target_tempo: 80`

        -   `User Profile` -> `seed_genres: "indie,orchestral"`, `seed_artists: "[ID for Arctic Monkeys],[ID for Radiohead],[ID for Hans Zimmer]"`

    -   For the 'Ambiental' mode, a similar mapping approach will be use but parameters will be defined at the music generation phase.

4.  **Song retrieval**
    -   **Spotify API Call** A query to `/v1/recommendations` is made with these combined parameters.
    -   **Generated Music Library Call** A query to the server is made, accesing the stored generated ambiental music.

## Evaluation Strategy

The system's effectiveness will be measured through a combination of user-driven and analytical methods:

-   **📈 User Engagement Metrics:** I will track the **skip rate** of recommended songs. A low skip rate is a strong indicator of a successful mood match.

-   **👍👎 Direct User Feedback:** A "like/dislike" mechanism on the current song will provide explicit feedback. Disliked songs can be used to create a negative seed list, preventing them from being recommended in the future.

-   **Reading Session Analysis:** Monitoring of the average session duration. Longer reading sessions could imply a more engaging and less disruptive experience.

## Observability Plan

To maintain a high-quality and stable service, I will focus on the following observability practices:

-   **📋 Structured Logging:** The .NET backend will log every request to the local LLM and the Spotify API, capturing key information like request parameters, response latency, and status codes for efficient debugging.

-   **🐞 Error Tracking:** I will set up monitoring to track the rate of errors from the application itself, the Spotify API (e.g., authentication issues, invalid requests) and any exceptions thrown by the LLM processing scripts. This will allow me to quickly identify and resolve systemic problems.
